---
title: "[CS50] 1.컴퓨팅 사고, Computational Thinking"
layout: single
author_profile: true
read_time: true
related: true
categories:
- Computer Science
tags:
- Study Note
- CS50
- 부스트코스
- 컴퓨터공학 이론
toc: true
toc_sticky: true
---


## "컴퓨터과학"

컴퓨터 과학이란 뭘까? David Malan 교수의 표현에 따르면 컴퓨터 과학은 **"단지 문제를 해결하는 과정"**에 대한 학문이다. 입력(문제)이 있을 때 출력(해답)을 만들어 내는 과정, 그 사이에 있는 것이 컴퓨터 과학이다.

이 과정에서 컴퓨터를 사용할 때 입력과 출력을 어떻게 표현할 것인지에 대해 공통적으로 합의된 내용을 알아야하기 때문에 우리가 컴퓨터의 작동원리 및 프로그래밍 언어를 배우는 것이다.

![(1)](https://raw.githubusercontent.com/lucathree/lucathree.github.io/master/assets/images/2021-06-28(1).png)

## 2진법

0과 1로만 정보를 표현하는 방법. 인간은 바로 1, 2, 3, 4, 5, 6, 7, 8, 9, 0 을 사용하는 10진법의 이해가 가능하지만 기계에게 정보를 알려주는 가장 간단한 방법은 스위치를 on 또는 off 상태로 바꿔주는 것이다.

컴퓨터에는 수많은 스위치, 즉 **트렌지스터**가 존재하고 트렌지스터를 사용하여 2진법으로 하나의 자릿수를 표현하는 단위를 **비트(bit -** binary digit**)**라고 한다.

그러나 하나의 비트만으로는 많은 정보를 표현할 수 없기 때문에 8개의 비트를 사용한 비트열, **바이트(byte)**를 사용하며 하나의 바이트로 **2^8 = 256**가지 표현이 가능하다.

> 🤔 <span style="color:gray">볼수록 컴퓨터와 사람의 뇌가 굉장히 유사하다는 생각이 든다. 인간은 10진법을 바로 이해할 수 있다고는 하지만 뇌의 작동원리로 따졌을 때 뉴런도 트렌지스터와 마찬가지로 전기신호를 쏘냐 안 쏘냐, all or nothing의 형태로 동작하기 때문에 사실은 우리도 인지하지 못한 채 2진법으로 생각과 감정등을 처리하고 있을지도 모른다.</span>

*** 더 알아보기**

- 왜 1바이트는 8개의 비트를 사용할까? 7bit, 9bit도 아니고 8bit가 1byte로 정해진 이유는 영문권에서 ASCII 코드를 사용해서 1byte 안에 문자를 저장하고 출력하기 위해 7bit + 1bit가 적당했기 때문이라고 한다. 실제로 4bit, 6bit 컴퓨터도 존재했는데 시간이 지나며 결국 8bit로 통일된 것이다. 1byte는 처음부터 추상적인 단위였고, 표준 C언어에서도 '8비트 *이상*'을 1byte로 삼도록 규정하고 있다.

## 정보의 표현

2진법을 통해 숫자나 문자에 값을 부여하여 표현을 할 수 있게 되었고, 미국인들은 어떤 숫자가 어떤 문자를 표현할 것인지에 대한 규칙으로 **ASCII (아스키코드/American Standard Code for Information Interchange)**를 사용하기로 했다.

- 아스키코드에서 알파벳 대문자를 표현하기 위해 사용하는 숫자 값. A는 10진수로 65, 2진수로는 10000001 이다.

![(2)](https://raw.githubusercontent.com/lucathree/lucathree.github.io/master/assets/images/2021-06-28(2).png)

하지만 전세계적으로는 알파벳 외에도 다양한 문자들이 사용되고 있기 때문에 그 뒤에 나온 표준인 **Unicode**로 더 많은 비트를 사용하여 이모티콘까지도 표현이 가능하게 되었다.

문자 외에도 색을 저장하기 위해서 빨강, 초록, 파랑 각각에서 255가지씩의 색상을 조합하여 RGB(255, 255, 255) 형태로 색의 값을 표현할 수도 있고, 이런 색상값을 픽셀이라는 단위로 담은 뒤 여러 픽셀들을 한꺼번에 나열함으로써 그림이나 영상을 표현할 수도 있다.

결론적으로 컴퓨터는 이런식으로 모든 정보를 0또는1의 조합으로 표현한다.

*** 더 알아보기**

- 😂 이모지는 유니코드에서의 값이 10진법으로 128,514이고 2진법으로는 11111011000000010다. 값 자체를 표현하는 것만에도 17비트, 2바이트 이상이 필요하다. 1byte에 값을 담을 수 있는 ASCII 알파벳과는 다르게 이렇게 큰 값은 여러 바이트를 연결해서 정보를 저장한다.

  이 때, 이런 다양한 문자를 효율적으로 저장하기 위해서 **문자 인코딩**을 사용한다. 대표적으로 **UTF-8(8-bit Unicode Transformation Format)** 같은 것이 문자 인코딩 방식인데, 유니코드를 8비트 단위로 변환하여 저장시키는 것을 의미한다.

  이게 무슨소리냐면, ASCII 문자들은 1바이트만 사용하여 저장하는 것이 가능한데 유니코드에서 'A'라는 문자를 저장하기 위해 4바이트를 사용한다면 저장용량을 낭비하는 꼴이 된다. 그래서 저장하려는 문자의 크기에 따라 8비트 단위로 구분하여 필요한 바이트 수만 사용할 수 있도록 유니코드 값을 수정해주는 것이 UTF-8 인코딩이다.

## 알고리즘

앞의 부분들이 컴퓨터에 정보를 입력하는 방법에 해당했다면, 출력을 하기 위해서는 **입력 받은 값을 출력형태로 만드는 처리 과정**이 필요한데 이 처리과정 자체가 **알고리즘**이다. 이를 조금 더 개념적으로 설명하자면, 알고리즘은 입력값을 출력값의 형태로 바꾸기 위해 어떤 명령들이 수행되어야 하는지에 대한 **규칙들의 순서적 나열**이다.

![(3)](https://raw.githubusercontent.com/lucathree/lucathree.github.io/master/assets/images/2021-06-28(3).png)

그 규칙들을 어떻게 나열하는 지에 따라 알고리즘의 종류가 달라지고, 같은 출력값을 얻더라도 알고리즘에 따라 출력하기까지의 시간이 달라질 수 있다. 시간이 늘어난다는 것은 컴퓨터가 들여야하는 노력, 규칙에 따른 단계가 많아진다는 뜻이고 이런 노력을 줄이기 위해서 알고리즘은 **정확성**도 중요하지만 **효율성**도 굉장히 중요하다.
