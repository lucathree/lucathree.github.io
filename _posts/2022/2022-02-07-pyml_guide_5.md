---
title: "[파이썬 머신러닝 완벽 가이드] 평가 (Evaluation)"
excerpt: "권철민님의 '파이썬 머신러닝 완벽 가이드' 강의를 기반으로 공부한 내용의 기록 - 3장 '평가' 정리"
layout: single
author_profile: true
read_time: true
related: true
categories:
- ML/DL
tags:
- Machine Learning
- Study Note
---



# 분류성능 평가지표

- 정확도 (Accuracy)
- 오차행렬 (Confusion Matrix)
- 정밀도 (Precision)
- 재현율 (Recall)
- F1 스코어 (정밀도+재현율)
- ROC AUC

## 정확도

— 전체 데이터 대비 예측 결과가 맞는 데이터의 비율을 %로 표현

- 직관적으로 모델 예측 성능을 나타내는 평가 지표
- 하지만 이진 분류(True or False)에서는 모델 성능을 왜곡할 수 있기 때문에 정확도만 가지고 성능을 평가하지는 않는다. 특히, 불균형한 레이블 값 분포에서 적합한 평가 지표가 되지 못한다.

## 오차행렬 (Confusion Matrix)

— 이진분류의 예측 오류가 얼마인지와 더불어 어떤 유형의 예측 오류(Type I, II Error)가 발생하고 있는 지를 나타내는 지표

- [[TN, FP], [FN, TP]] 형태로 결과 반환
- False Positive, 실제로는 거짓일 때 참을 반환한 경우 → Type I Error
- False Negative, 실제로는 참인데 거짓을 반환한 경우 → Type II Error
- 정확도 = 예측 결과와 실제 값이 동일한 건수/전체 데이터 수 = (TN + TP)/(TN + FP + FN + FP)

```python
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, pred)
```

## 정밀도 & 재현율

### 정밀도 (Precision)

— 예측을 Positive로 한 대상 중 예측과 실제 값이 Positive로 일치한 데이터의 비율

⇒ **정밀도 = TP / (FP + TP)**

- precision_score() 메서드로 확인

### 재현율 (Recall)

— 실제 값이 Positive인 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율

⇒ **재현율 = TP / (FN + TP)**

- recall_score() 메서드로 확인

### 정밀도와 재현율의 트레이드오프

— 업무에 따라 정밀도와 재현율의 중요도는 상대적일 수 있다

- 정밀도가 중요한 경우 : 스팸 메일을 거르는 시스템의 경우 정상 메일(Negative)을 스팸(Positive)으로 분류하면 안된다. 즉, False Positive 혹은 Type I Error를 일으키면 안되는 경우 정밀도가 더 중요한 지표가 된다.
- 재현율이 중요한 경우 : 암 진단 처럼 실제로 양성(Positive)인 결과를 음성(Negative)로 판별해버리는 경우가 더 치명적인 경우. 즉, False Negative 혹은 Type II Error를 일으키면 안되는 경우 재현율이 더 중요한 지표가 된다.
- 분류 업무 특성상 위 처럼 정밀도 또는 재현율이 특별히 강조돼야 할 경우 **분류 결정 임계값 (Threshold)**을 조정해 수치를 높일 수 있다.
  - Estimator 객체의 predict_proba() 메소드로 분류 결정 예측 확률 반환
  - 임계값이 낮아질 수록 Positive로 예측할 확률이 높아진다 → **재현율 증가**
- 정밀도와 재현율은 상호 보완적인 평가 지표이기 때문에 어느 한쪽을 강제로 높이면 다른 하나의 수치가 떨어지기 쉽다. 이를 정밀도/재현율의 트레이드오프(Trade-off)라고 한다.
  - precision_recall_curve() 함수를 통해 임계값에 따른 정밀도, 재현율의 변화값 확인

## F1 Score

**정밀도와 재현율의 맹점 :** 임계값을 필요에 따라 조정하면 얼마든지 정밀도 또는 재현율을 100%로 조작할 수 있다. 즉, 두 지표의 밸런스를 확인할 필요가 있다.

⇒ **F1 스코어**는 정밀도와 재현율을 결합한 지표로 정밀도와 재현율이 어느 한쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 갖는다.

- 사이킷런에서 f1_score() 함수로 확인 가능

## ROC 곡선과 AUC

### **ROC 곡선 (Receiver Operation Characteristic Curve)**

— FPR(False Positive Rate)이 변할 때 TPR(True Positive Rate, =재현율)이 어떻게 변하는지 나타내는 곡선

- TPR, 즉 재현율은 "민감도"로도 불린다

- roc_curve(y_true, y_score)

  - y_true : 실제 클래스 값
  - y_score : predict_proba()의 반환 값 중 positive 예측 확률을 사용

  → fpr, tpr, thresholds 반환

### AUC (Area Under Curve)

— ROC곡선 아래의 면적을 구한 것. 면적이 1에 가까울 수록 좋은 수치

- roc_auc_score(y_true, y_score)

  - 사용되는 파라미터는 roc_curve()와 동일

  → AUC 스코어 값 반환
