---
title: "[파이썬 머신러닝 완벽 가이드] 분류 (Classification), 결정트리 (Decision Tree)"
excerpt: "권철민님의 '파이썬 머신러닝 완벽 가이드' 강의를 기반으로 공부한 내용의 기록 - 4장 '분류' 정리 (1)"
layout: single
author_profile: true
read_time: true
related: true
categories:
- ML/DL
tags:
- Machine Learning
- Study Note
---



# 분류

— 머신러닝에서 분류란 주어진 데이터의 피처와 레이블 값을 머신러닝 알고리즘으로 학습하고 모델을 생성하여 새로운 데이터(피처)가 주어졌을 때 해당 데이터의 레이블 값이 무엇인지 예측하도록 하는 것이다.

## 대표알고리즘

- 나이브 베이즈 (Naïve Bayes) : 특성들 사이의 독립을 가정하는 베이즈 정리를 기반으로 한 알고리즘
- 로지스틱 회귀 (Logistic Regression) : 회귀 알고리즘이지만 시그모이드 함수를 사용하여 분류에도 사용이 가능. 독립 변수와 선형 관계성에 기반한다.
- 결정트리 (Decision Tree) : 데이터 균일도에 따른 규칙을 기반으로 하는 알고리즘
- 서포트 벡터 머신 (Support Vector Machine, SVM) : 개별 클래스 간 최대 분류 마진을 효과적으로 잡아줌
- 최소 근접 (Nearest Neighbor) : 근접 거리를 기준으로 하는 알고리즘
- 신경망 (Neural Network)
- 앙상블 (Ensemble) : 개별 알고리즘들의 결합으로 만든 알고리즘



# 결정트리

— "규칙이 있는 스무고개"

- 나무의 가지가 뻗어 나가듯이 데이터에 있는 규칙을 학습을 통해 찾아내어 분류 규칙을 생성하고, 분류 규칙마다 가지가 분할된다.
- 분류의 시작점을 **"루트 노드"**라 하고, 규칙 조건에 의해 분할이 일어나는 구간을 **"규칙 노드"**, 분류값이 결정되어 더 이상 분할되지 않는 구간을 **"리프 노드"**라 한다.
- 어떤 기준을 바탕으로 규칙을 만드는가가 알고리즘의 성능을 크게 좌우한다.  

- 데이터 스케일링이나 정규화 같은 전처리가 결과에 미치는 영향이 매우 적다.

- 데이터 분류 과정을 눈으로 볼 수 있어 직관적인 것이 장점.

- 하지만 규칙이 복잡해질수록 과적합이 일어나기 쉽고 예측 성능이 저하된다는 단점이 존재한다.

  ⇒ 단점 극복을 위해 트리의 크기를 사전에 제한하는 튜닝을 필요로 한다.

**그런데,** 앙상블에 사용하기에는 오히려 좋다!

- **앙상블** : 여러개의 <u>약한 학습기를 결합</u>해 확률적 보완과 가중치를 업데이트하며 예측 성능을 향상시킨다. (GBM, XGBoost, LightGBM 등) 

  

## 트리분할을 위한 데이터의 균일도

결정트리는 규칙 생성을 위한 기준으로 데이터의 균일도를 이용한다. 특정 조건 아래에 균일한 데이터가 가장 많은 경우 해당 조건을 데이터 분류를 위한 규칙으로 사용하게 된다.

**균일도 측정 방법**

- 정보 이득 (Information Gain) : 엔트로피(혼잡도)를 기준으로 1 - (엔트로피) 값을 정보 이득 지수로 사용. 정보 이득 지수가 높을수록, 즉 데이터가 덜 혼잡할수록 균일한 것으로 추정

- 지니 계수 : 경제학에서 불평등 지수를 나타내는 지니 계수를 사용. 계수가 0에 가까울수록 평등하고 1에 가까울수록 불평등하다. 머신러닝에선 지니 계수가 낮을수록 데이터가 균일하다고 판단, 계수가 낮은 속성을 기준으로 분할한다.

    

## 규칙 노드 생성 프로세스

- 데이터 집합의 모든 아이템이 같은 분류에 속하는지 확인
  - If True : 리프 노드 생성, 분류 결정.
  - Else : 데이터 분할 기준 탐색 (정보 이득 or 지니 계수 사용)
    - 가장 적합한 기준으로 데이터를 분할하여 규칙 브랜치/노드 생성
- 모든 데이터의 분류가 결정 될 때까지 위 과정 반복



## 결정 트리 파라미터

사이킷런의 DecisionTreeClassifier 클래스 기준 주요 파라미터

### min_samples_split

- 노드를 분할하기 위한 최소한의 샘플 데이터 수. 과적합을 제어하는 데 사용된다.
- 디폴트는 2. 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성이 증가

###  min_samples_leaf

- 말단 노드가 되기 위한 최소한의 샘플 데이터 수
- min_samples_split과 유사하지만 비대칭적인 데이터의 경우 특정 클래스의 데이터가 극도로 작은 경우가 있을 수 있으므로 이런 경우에는 작게 설정할 필요가 있다.

### max_features

- 최적의 분할을 위해 고려할 최대 피쳐 개수. 디폴트는 None이며 데이터셋의 모든 피처를 사용해서 분할을 수행한다.
- int로 지정할 경우 피쳐의 개수, float으로 지정할 경우 전체 피쳐 중 사용할 피쳐의 퍼센트.

- 'sqrt'는 sqrt(전체 피쳐 개수) 만큼 선정. 'auto'로 해도 동일
- 'log'는 log2(전체 피쳐 개수) 선정

### max_depth

- 트리의 최대 깊이를 규정
- 디폴트는 None이며 모든 데이터가 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드의 데이터 개수가 min_samples_split 보다 작아질 때까지 계속 깊이를 증가시킨다.
- 깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합이 일어날 수 있으므로 제어 필요.

### max_leaf_nodes

- 말단 노드의 최대 개수 설정
